{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "F5D2bn6r-nH1"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtwgLw-r-nG4"
      },
      "source": [
        "# Demonstracija korištenja neuronskih mreža"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVF4Qukz-nG-"
      },
      "source": [
        "U ovoj demonstraciji ćemo istražiti kako se neuronske mreže mogu iskoristiti za rješavanje problema klasifikacije podataka. Koristiti ćemo PyTorch modul za Python, za što će Vam trebati Python 3.8+ i PyTorch 1.6+. Također je potrebno instalirati pakete `numpy`, `opencv-python`, `wandb`, `matplotlib`, `torchsummary` i `tqdm`.\n",
        "\n",
        "Upoznavati ćemo komponente PyTorcha korak po korak, a za one koji žele znati više se preporuča sljedeći tutorial:  https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoLF8ap4HgC2"
      },
      "source": [
        "Za postavljanje radne okoline, pokrenite sljedeće naredbe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLxQcpWdHbfJ"
      },
      "source": [
        "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install torchsummary\n",
        "!pip install numpy matplotlib opencv-python\n",
        "!pip install tqdm wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7nN4JHz-nHA"
      },
      "source": [
        "Za početak importamo sve module koje ćemo koristiti za demonstraciju."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjkOA99k-nHB"
      },
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "\n",
        "import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGvNCIlJ-nHD"
      },
      "source": [
        "Weights & Biases je online sustav za rukovanje sa eksperimentima strojnog i dubokog učenja. Korisnički račun si možete napraviti besplatno na njihovoj stranici ukoliko želite koristiti taj (opcionalni, ali preporučeni) dio vježbe. Nakon što napravite račun, pod \"Profilna slika\" -> \"Settings\" -> \"API keys\" možete napraviti API ključ za pristup Vašem računu. Taj API ključ zapišite u \"wandb_key.env\" datoteku."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcX1rG6m-nHD"
      },
      "source": [
        "with open(\"wandb_key.env\", 'r') as f:\n",
        "    os.environ['WANDB_API_KEY'] = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhQr_ie8-nHE"
      },
      "source": [
        "## Eksperimenti sa neuronskim mrežama nad točkama i clusterima"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBJNAa8X-nHE"
      },
      "source": [
        "Prvi problem koji ćemo pokušati riješiti je sljedeći: možemo li za neku novu točku reći kojem clusteru pripada?\n",
        "\n",
        "Što to točno znači pogledajmo u sljedećem kodu gdje generiramo dataset. Generirati ćemo `num_of_clusters` clustera. \"Cluster\" se definira kao \"a group of similar things or people positioned or occurring closely together\". Iako bi prva ideja bila samo računanje udaljenosti od centra svakog clustera, to ne bi pokrilo sve slučajeve koje možemo sresti u podacima. Npr. neki clusteri mogu biti izduženi samo u jednoj dimenziji, što naivni model ne bi uspješno rješio.\n",
        "\n",
        "Primjetimo i `train_test_ratio` varijablu. Pomoću nje određujemo nad kolikim postotkom podataka ćemo mreže učiti, a nad kolikim postotkom podataka ćemo mrežu evaluirati. Prisjetimo se, **želimo napraviti model koji dobro radi na neviđenim podacima**. Da bi to testirali, kod pripreme podataka odvajamo podskup koji će bit neviđen tijekom treniranja.\n",
        "\n",
        "Clustere generiramo tako da je svaki cluster jedna normalna distribucija, a uzorci su uzorci i te normalne distribucije."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1G_RKyE-nHG"
      },
      "source": [
        "num_of_clusters = 5\n",
        "num_of_samples = 50\n",
        "train_test_ratio = 0.8\n",
        "\n",
        "mean_xs = np.random.uniform(0, 10 * num_of_clusters, num_of_clusters).astype(np.uint8).astype(np.float32)\n",
        "std_xs = np.random.uniform(0, num_of_clusters, num_of_clusters)\n",
        "\n",
        "mean_ys = np.random.uniform(0, 10 * num_of_clusters, num_of_clusters).astype(np.uint8).astype(np.float32)\n",
        "std_ys = np.random.uniform(0, num_of_clusters, num_of_clusters)\n",
        "\n",
        "number_of_train_samples = int(num_of_clusters * num_of_samples * train_test_ratio)\n",
        "number_of_test_samples = num_of_clusters * num_of_samples - number_of_train_samples\n",
        "\n",
        "data_x_train = []\n",
        "data_y_train = []\n",
        "labels_train = []\n",
        "\n",
        "data_x_test = []\n",
        "data_y_test = []\n",
        "labels_test = []\n",
        "\n",
        "for idx in range(0, num_of_clusters):\n",
        "    data_x_train.extend(np.random.normal(mean_xs[idx], std_xs[idx], number_of_train_samples // num_of_clusters))\n",
        "    data_y_train.extend(np.random.normal(mean_ys[idx], std_ys[idx], number_of_train_samples // num_of_clusters))\n",
        "    labels_train.extend([idx] * (number_of_train_samples // num_of_clusters))\n",
        "\n",
        "    data_x_test.extend(np.random.normal(mean_xs[idx], std_xs[idx], number_of_test_samples // num_of_clusters))\n",
        "    data_y_test.extend(np.random.normal(mean_ys[idx], std_ys[idx], number_of_test_samples // num_of_clusters))\n",
        "    labels_test.extend([idx] * (number_of_test_samples // num_of_clusters))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV3x1OpT-nHH"
      },
      "source": [
        "Sljedeći kod nam vizualizira podatke koje smo generirali.\n",
        "\n",
        "Napomena: Ukoliko Vam se desi da imate dva ili više clustera koji se jako preklapaju, preporučamo da ponovo pokrene gornji kod. Ponovite to dok niste zadovoljni sa razmakom clustera."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3IjHp_e-nHJ"
      },
      "source": [
        "for idx in range(0, num_of_clusters):\n",
        "    plt.scatter(data_x_train[(idx)*number_of_train_samples//num_of_clusters:(idx+1)*number_of_train_samples//num_of_clusters], data_y_train[(idx)*number_of_train_samples//num_of_clusters:(idx+1)*number_of_train_samples//num_of_clusters], label=idx)\n",
        "plt.legend()\n",
        "plt.title(\"Samples from the train set\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwkbMW9U-nHM"
      },
      "source": [
        "Pogledajmo i skup za testiranje. Možemo vidjeti da se ne radi o istim točkama, da ih je manje, ali da su istog karaktera kao primjeri iz train seta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oes0RIFD-nHO"
      },
      "source": [
        "for idx in range(0, num_of_clusters):\n",
        "    plt.scatter(data_x_test[(idx)*number_of_test_samples//num_of_clusters:(idx+1)*number_of_test_samples//num_of_clusters], data_y_test[(idx)*number_of_test_samples//num_of_clusters:(idx+1)*number_of_test_samples//num_of_clusters], label=idx)\n",
        "plt.legend()\n",
        "plt.title(\"Samples from the test set\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hyL3PVP-nHO"
      },
      "source": [
        "Pripremimo skup podataka. Da bi se podaci mogli koristiti u PyTorchu, potrebno je pripremiti DataLoader s Datasetom tih podataka. Prvo naše podatke spremamo u 'torch.Tensor' (ekvivalent matrice u numpy). Iz tih tenzora generiramo `TensorDataset`, koji radi točno što mu ime kaže - od `Tensor` radi `Dataset`. `DataLoader` prima kao prvi argument `Dataset`, a ostali argumenti definiraju ponašanje. U ovom slučaju, `batch_size`, koji utjeće na veličinu mini-skupine, i `shuffle` koji mjenja naš `DataLoader` na način da iteriranjem kroz njega ne dobivamo uvijek isti poredak uzoraka."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKtri5_H-nHO"
      },
      "source": [
        "batch_size = 1\n",
        "\n",
        "tensor_x_train = torch.Tensor(np.dstack([data_x_train, data_y_train]).reshape(number_of_train_samples, 2).astype(np.float32)) # komentirat zašto stackam\n",
        "tensor_y_train = torch.Tensor(labels_train).to(dtype=torch.int64)\n",
        "\n",
        "train_dataset = TensorDataset(tensor_x_train, tensor_y_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "tensor_x_test = torch.Tensor(np.dstack([data_x_test, data_y_test]).reshape(number_of_test_samples, 2).astype(np.float32))\n",
        "tensor_y_test = torch.Tensor(labels_test).to(dtype=torch.int64)\n",
        "\n",
        "test_dataset = TensorDataset(tensor_x_test, tensor_y_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDX-wQco-nHO"
      },
      "source": [
        "Definirajmo sad jednostavnu mrežu. Da bi u PyTorchu definirali mrežu, radimo klasu koja nasljeđuje `nn.Module`. Za implementaciju modela je potrebno implementirati dvije metode: konstruktor `__init__(self)` i metodu `forward(self, ...)`.\n",
        "\n",
        "Želimo jednoslojnu neuronsku mrežu, koja prima koordinate točke koje provjeravamo (znači 2 broja) i vraća vjerojatnost pripadnosti za svaku klasu (cluster u ovom slučaju). Pogledajmo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhKeNIUW-nHP"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQtI9N1r-nHP"
      },
      "source": [
        "Sad kad imamo model, pripremimo se za treniranje. Prvo ćemo definirati koliko epoha treniramo (`n_epochs`), kolika je stopa učenja (`learning_rate`) i na kojem uređaju ćemo trenirati našu mrežu (`device`). Za velike neurnske mreže se treniranje radi na GPU ili TPU, ali za školske primjere je CPU dovoljan.\n",
        "\n",
        "Nakon toga instanciramo našu mrežu, i pošaljemo je na uređaj na kojem će se trenirati (`.to(device)`). Također će nam trebati optimizator. U PyTorchu se optimizator nalazi u `optim` podmodulu. Koristiti ćemo osnovni Stohastic Gradient Descent (\"SGD\").\n",
        "\n",
        "Metoda `summary(model, input_size, device)` nam ispisuje kako naš model izgleda. Za svaki sloj, ispisuje njegovu dimenziju i od koliko se parametara sastoji taj sloju."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5W5Cma8-nHY"
      },
      "source": [
        "n_epochs = 5\n",
        "learning_rate = 1\n",
        "device = 'cpu'\n",
        "\n",
        "network = Net().to(device)\n",
        "optimizer = optim.SGD(network.parameters(), lr=learning_rate)\n",
        "summary(network, input_size=(2, ), device=device) # komentar šta to je"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1CRMup9-nHZ"
      },
      "source": [
        "Došlo je vrijeme za treniranje. Prvo instanciramo eksperiment za Weights & Biases. `wandb.watch(model, ...)` će pratiti model tijekom treniranja, te će nam dati duboki uvid u proces učenja.\n",
        "\n",
        "Nakon toga pripremamo liste u kojima ćemo čuvati rezultate tijekom treniranja. Te rezultate ćemo kasnije vizualizirati.\n",
        "\n",
        "Tad, za svaku epohu, prolazimo kroz svaki mini-skup našeg `DataLoader` skupa za učenje, šaljemo podatke na odgovarajući uređaj, uklanjamo sve gradijente u našoj mreži (ako je nešto ostalo od prije), izvršimo našu mrežu nad tim podacima, računamo pogrešku, provodimo backpropagaciju te izvršavamo korak optimizacije.\n",
        "\n",
        "Nakon što odradimo korake optimizacije za cijeli skup za učenje, potrebno je evalurati na neviđenim podacima. Radi toga model prebacujemo u `.eval()` stanje, i sa `with torch.no_grad()` određujemo scope u kojem operacije nad mrežom neće računati gradijente. Slanje podataka i evaluacija se radi na isti način kao i tijekom treniranja, a nad rezultatima računamo metrike koje nas zanimaju. Kako se radi o klasifikacijom problemu, u ovom slučaju su to loss i accuracy.\n",
        "\n",
        "wandb će ispisati link na kojem možete direktno pratiti napredak modela, i gdje možete dobit uvid u vrijednosti gradijenta tijekom treniranja."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNwpph-a-nHZ"
      },
      "source": [
        "run = wandb.init(project=\"oi_demo\", reinit=True)\n",
        "wandb.watch(network, log_freq=1)\n",
        "\n",
        "loss_acc = []\n",
        "test_loss_acc = []\n",
        "test_accuracy_acc = []\n",
        "train_steps = []\n",
        "test_steps = []\n",
        "current_step = 0\n",
        "\n",
        "for epoch_idx in range(0, n_epochs):\n",
        "    network.train()\n",
        "\n",
        "    per_epoch_trainloss = []\n",
        "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        network.zero_grad()\n",
        "        output = network(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "\n",
        "        if epoch_idx == 0:\n",
        "            if batch_idx == 0:\n",
        "                print(\"# Epoha 1, batch 0\")\n",
        "                print(\"## Sloj 1 u 1. koraku:\")\n",
        "                print(\"### Tezine\")\n",
        "                print(network.fc1.weight)\n",
        "                print(\"### Tezine - Gradient\")\n",
        "                print(network.fc1.weight.grad)\n",
        "\n",
        "                print(\"### Bias\")\n",
        "                print(network.fc1.bias)\n",
        "                print(\"### Bias - Gradient\")\n",
        "                print(network.fc1.bias.grad)\n",
        "\n",
        "            if batch_idx == 1:\n",
        "                print(\"--------------\")\n",
        "                print(\"# Epoha 1, batch 1\")\n",
        "                print(\"## Sloj 1 u 1. koraku:\")\n",
        "                print(\"### Tezine\")\n",
        "                print(network.fc1.weight)\n",
        "\n",
        "                print(\"### Bias\")\n",
        "                print(network.fc1.bias)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_acc.append(loss.item())\n",
        "        current_step += 1\n",
        "        train_steps.append(current_step)\n",
        "        per_epoch_trainloss.append(loss.item())\n",
        "\n",
        "\n",
        "    network.eval()\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        for data, target in test_dataloader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            output = network(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "\n",
        "    test_loss /= len(test_dataloader.dataset)\n",
        "    accuracy = 100. * correct / len(test_dataloader.dataset)\n",
        "\n",
        "    test_loss_acc.append(test_loss)\n",
        "    test_accuracy_acc.append(accuracy)\n",
        "    test_steps.append(current_step)\n",
        "\n",
        "    wandb.log({\n",
        "        'test_loss': test_loss,\n",
        "        'test_accuracy': accuracy,\n",
        "        'train_loss': np.mean(per_epoch_trainloss)\n",
        "    })\n",
        "\n",
        "run.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I49wSq-Y-nHZ"
      },
      "source": [
        "Vizualizirajmo rezultate. `_step` i `_acc` liste uvijek dolaze u paru, gdje `_step` lista opisuje o kojem koraku se radi, dok `_acc` lista sadrži vrijednost naše metrike za taj korak."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96_bcNMB-nHZ"
      },
      "source": [
        "rolling_average_width_percentage = 0.05\n",
        "plt.figure(figsize=(16, 4))\n",
        "plt.plot(train_steps, loss_acc, label=\"Train loss\")\n",
        "plt.plot(test_steps, test_loss_acc, label=\"Test loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVyHnQlF-nHa"
      },
      "source": [
        "Train vrijednosti ima puno, jer se bilježi za svaki mini-skup. Pogledajmo samo test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW_dTJB6-nHa"
      },
      "source": [
        "plt.figure(figsize=(16, 4))\n",
        "plt.plot(test_steps, test_accuracy_acc, label=\"Test accuracy\")\n",
        "plt.ylabel(\"Accuracy [%]\")\n",
        "plt.xlabel(\"Training step\")\n",
        "plt.grid()\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-lfdG_b-nHb"
      },
      "source": [
        "To je osnovni princip rada s neuronskim mrežama u PyTorchu. U ostatku demonstracije se koristi isti okvir, samo s različitim parametrima ovisno o tome što želimo vidjeti."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYBaUoNa-nHb"
      },
      "source": [
        "Provjerimo sada kako bi se naš model ponašao ako povećamo `batch_size` na vrijednost 4.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xdQXj8u-nHb"
      },
      "source": [
        "batch_size = 4\n",
        "\n",
        "tensor_x_train = torch.Tensor(np.dstack([data_x_train, data_y_train]).reshape(number_of_train_samples, 2).astype(np.float32))\n",
        "tensor_y_train = torch.Tensor(labels_train).to(dtype=torch.int64)\n",
        "\n",
        "train_dataset = TensorDataset(tensor_x_train, tensor_y_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "tensor_x_test = torch.Tensor(np.dstack([data_x_test, data_y_test]).reshape(number_of_test_samples, 2).astype(np.float32))\n",
        "tensor_y_test = torch.Tensor(labels_test).to(dtype=torch.int64)\n",
        "\n",
        "test_dataset = TensorDataset(tensor_x_test, tensor_y_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGcBnPVT-nHc"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_ZcVxB8-nHc"
      },
      "source": [
        "n_epochs = 5\n",
        "learning_rate = 1\n",
        "device = 'cpu'\n",
        "\n",
        "network = Net().to(device)\n",
        "optimizer = optim.SGD(network.parameters(), lr=learning_rate)\n",
        "summary(network, input_size=(2, ), device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omBvnhQo-nHc"
      },
      "source": [
        "run = wandb.init(project=\"oi_demo\", reinit=True)\n",
        "wandb.watch(network, log_freq=1)\n",
        "\n",
        "loss_acc = []\n",
        "test_loss_acc = []\n",
        "test_accuracy_acc = []\n",
        "train_steps = []\n",
        "test_steps = []\n",
        "current_step = 0\n",
        "\n",
        "for epoch_idx in range(0, n_epochs):\n",
        "    network.train()\n",
        "\n",
        "    per_epoch_trainloss = []\n",
        "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        network.zero_grad()\n",
        "        output = network(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_acc.append(loss.item())\n",
        "        current_step += 1\n",
        "        train_steps.append(current_step)\n",
        "        per_epoch_trainloss.append(loss.item())\n",
        "\n",
        "    network.eval()\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        for data, target in test_dataloader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            output = network(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "\n",
        "    test_loss /= len(test_dataloader.dataset)\n",
        "    accuracy = 100. * correct / len(test_dataloader.dataset)\n",
        "\n",
        "    test_loss_acc.append(test_loss)\n",
        "    test_accuracy_acc.append(accuracy)\n",
        "    test_steps.append(current_step)\n",
        "\n",
        "    wandb.log({\n",
        "        'test_loss': test_loss,\n",
        "        'test_accuracy': accuracy,\n",
        "        'train_loss': np.mean(per_epoch_trainloss)\n",
        "    })\n",
        "\n",
        "run.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qGlQNKX-nHd"
      },
      "source": [
        "rolling_average_width_percentage = 0.05\n",
        "plt.figure(figsize=(16, 4))\n",
        "plt.plot(train_steps, loss_acc, label=\"Raw train loss\")\n",
        "plt.plot(test_steps, test_loss_acc, label=\"Raw test loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldOPi5Df-nHd"
      },
      "source": [
        "plt.figure(figsize=(16, 4))\n",
        "plt.plot(test_steps, test_accuracy_acc, label=\"Test accuracy\")\n",
        "plt.ylabel(\"Accuracy [%]\")\n",
        "plt.xlabel(\"Training step\")\n",
        "plt.grid()\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3mZssrJ-nHd"
      },
      "source": [
        "Vidimo drugačije ponašanje! Iz ponašanja lossa vidimo da mreža zapravo ne napreduje, nego većinom \"titra\" oko neke fiksne vrijednosti. To je često indikator da nam je stopa učenja prevelika. Smanjimo je na 0.01!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aVBZsBm-nHe"
      },
      "source": [
        "batch_size = 4\n",
        "\n",
        "tensor_x_train = torch.Tensor(np.dstack([data_x_train, data_y_train]).reshape(number_of_train_samples, 2).astype(np.float32))\n",
        "tensor_y_train = torch.Tensor(labels_train).to(dtype=torch.int64)\n",
        "\n",
        "train_dataset = TensorDataset(tensor_x_train, tensor_y_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "tensor_x_test = torch.Tensor(np.dstack([data_x_test, data_y_test]).reshape(number_of_test_samples, 2).astype(np.float32))\n",
        "tensor_y_test = torch.Tensor(labels_test).to(dtype=torch.int64)\n",
        "\n",
        "test_dataset = TensorDataset(tensor_x_test, tensor_y_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L28ILXuN-nHe"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsP3QGr9-nHe"
      },
      "source": [
        "n_epochs = 5\n",
        "learning_rate = 0.01\n",
        "device = 'cpu'\n",
        "\n",
        "network = Net().to(device)\n",
        "optimizer = optim.SGD(network.parameters(), lr=learning_rate)\n",
        "summary(network, input_size=(2, ), device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQhZbmY1-nHe"
      },
      "source": [
        "run = wandb.init(project=\"oi_demo\", reinit=True)\n",
        "wandb.watch(network, log_freq=1)\n",
        "\n",
        "loss_acc = []\n",
        "test_loss_acc = []\n",
        "test_accuracy_acc = []\n",
        "train_steps = []\n",
        "test_steps = []\n",
        "current_step = 0\n",
        "\n",
        "for epoch_idx in range(0, n_epochs):\n",
        "    network.train()\n",
        "\n",
        "    per_epoch_trainloss = []\n",
        "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        network.zero_grad()\n",
        "        output = network(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_acc.append(loss.item())\n",
        "        current_step += 1\n",
        "        train_steps.append(current_step)\n",
        "        per_epoch_trainloss.append(loss.item())\n",
        "\n",
        "    network.eval()\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        for data, target in test_dataloader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            output = network(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "\n",
        "    test_loss /= len(test_dataloader.dataset)\n",
        "    accuracy = 100. * correct / len(test_dataloader.dataset)\n",
        "\n",
        "    test_loss_acc.append(test_loss)\n",
        "    test_accuracy_acc.append(accuracy)\n",
        "    test_steps.append(current_step)\n",
        "\n",
        "    wandb.log({\n",
        "        'test_loss': test_loss,\n",
        "        'test_accuracy': accuracy,\n",
        "        'train_loss': np.mean(per_epoch_trainloss)\n",
        "    })\n",
        "\n",
        "run.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJa4kEWw-nHe"
      },
      "source": [
        "rolling_average_width_percentage = 0.05\n",
        "plt.figure(figsize=(16, 4))\n",
        "plt.plot(train_steps, loss_acc, label=\"Raw train loss\")\n",
        "plt.plot(test_steps, test_loss_acc, label=\"Raw test loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p42ftOy1-nHe"
      },
      "source": [
        "plt.figure(figsize=(16, 4))\n",
        "plt.plot(test_steps, test_accuracy_acc, label=\"Test accuracy\")\n",
        "plt.ylabel(\"Accuracy [%]\")\n",
        "plt.xlabel(\"Training step\")\n",
        "plt.grid()\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfZ9eCiA-nHf"
      },
      "source": [
        "Vidimo opet drugačije ponašanje. Isprobajmo još manju stopu učenja, 0.0001."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux-kj2EG-nHf"
      },
      "source": [
        "batch_size = 4\n",
        "\n",
        "tensor_x_train = torch.Tensor(np.dstack([data_x_train, data_y_train]).reshape(number_of_train_samples, 2).astype(np.float32))\n",
        "tensor_y_train = torch.Tensor(labels_train).to(dtype=torch.int64)\n",
        "\n",
        "train_dataset = TensorDataset(tensor_x_train, tensor_y_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "tensor_x_test = torch.Tensor(np.dstack([data_x_test, data_y_test]).reshape(number_of_test_samples, 2).astype(np.float32))\n",
        "tensor_y_test = torch.Tensor(labels_test).to(dtype=torch.int64)\n",
        "\n",
        "test_dataset = TensorDataset(tensor_x_test, tensor_y_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNR4625q-nHf"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCBx-4hQ-nHg"
      },
      "source": [
        "n_epochs = 5\n",
        "learning_rate = 0.0001\n",
        "device = 'cpu'\n",
        "\n",
        "network = Net().to(device)\n",
        "optimizer = optim.SGD(network.parameters(), lr=learning_rate)\n",
        "summary(network, input_size=(2, ), device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yanJC8sk-nHg"
      },
      "source": [
        "run = wandb.init(project=\"oi_demo\", reinit=True)\n",
        "wandb.watch(network, log_freq=1)\n",
        "\n",
        "loss_acc = []\n",
        "test_loss_acc = []\n",
        "test_accuracy_acc = []\n",
        "train_steps = []\n",
        "test_steps = []\n",
        "current_step = 0\n",
        "\n",
        "for epoch_idx in range(0, n_epochs):\n",
        "    network.train()\n",
        "\n",
        "    per_epoch_trainloss = []\n",
        "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        network.zero_grad()\n",
        "        output = network(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_acc.append(loss.item())\n",
        "        current_step += 1\n",
        "        train_steps.append(current_step)\n",
        "        per_epoch_trainloss.append(loss.item())\n",
        "\n",
        "\n",
        "    network.eval()\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        for data, target in test_dataloader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            output = network(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "\n",
        "    test_loss /= len(test_dataloader.dataset)\n",
        "    accuracy = 100. * correct / len(test_dataloader.dataset)\n",
        "\n",
        "    test_loss_acc.append(test_loss)\n",
        "    test_accuracy_acc.append(accuracy)\n",
        "    test_steps.append(current_step)\n",
        "\n",
        "    wandb.log({\n",
        "        'test_loss': test_loss,\n",
        "        'test_accuracy': accuracy,\n",
        "        'train_loss': np.mean(per_epoch_trainloss)\n",
        "    })\n",
        "\n",
        "run.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz6Taatu-nHg"
      },
      "source": [
        "rolling_average_width_percentage = 0.05\n",
        "plt.figure(figsize=(16, 4))\n",
        "plt.plot(train_steps, loss_acc, label=\"Raw train loss\")\n",
        "plt.plot(test_steps, test_loss_acc, label=\"Raw test loss\")\n",
        "\n",
        "\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqf0Uuit-nHh"
      },
      "source": [
        "plt.figure(figsize=(16, 4))\n",
        "plt.plot(test_steps, test_accuracy_acc, label=\"Test accuracy\")\n",
        "plt.ylabel(\"Accuracy [%]\")\n",
        "plt.xlabel(\"Training step\")\n",
        "plt.grid()\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K83BRVYY-nHh"
      },
      "source": [
        "Možemo uočiti da loss pada, ali sporo. Moguće je da naš model nije dovoljan. Ponovimo naš eksperiment sa stopom učenja od 0.01, ali ovaj puta sa dva sloja."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lxeKATQ-nHh"
      },
      "source": [
        "batch_size = 4\n",
        "\n",
        "tensor_x_train = torch.Tensor(np.dstack([data_x_train, data_y_train]).reshape(number_of_train_samples, 2).astype(np.float32))\n",
        "tensor_y_train = torch.Tensor(labels_train).to(dtype=torch.int64)\n",
        "\n",
        "train_dataset = TensorDataset(tensor_x_train, tensor_y_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "tensor_x_test = torch.Tensor(np.dstack([data_x_test, data_y_test]).reshape(number_of_test_samples, 2).astype(np.float32))\n",
        "tensor_y_test = torch.Tensor(labels_test).to(dtype=torch.int64)\n",
        "\n",
        "test_dataset = TensorDataset(tensor_x_test, tensor_y_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeT2cDY3-nHi"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 2)\n",
        "        self.fc2 = nn.Linear(2, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQNYXv7a-nHi"
      },
      "source": [
        "n_epochs = 5\n",
        "learning_rate = 0.01\n",
        "device = 'cpu'\n",
        "\n",
        "network = Net().to(device)\n",
        "optimizer = optim.SGD(network.parameters(), lr=learning_rate)\n",
        "summary(network, input_size=(2, ), device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcpwkT6w-nHj"
      },
      "source": [
        "run = wandb.init(project=\"oi_demo\", reinit=True)\n",
        "wandb.watch(network, log_freq=1)\n",
        "\n",
        "loss_acc = []\n",
        "test_loss_acc = []\n",
        "test_accuracy_acc = []\n",
        "train_steps = []\n",
        "test_steps = []\n",
        "current_step = 0\n",
        "\n",
        "for epoch_idx in range(0, n_epochs):\n",
        "    network.train()\n",
        "\n",
        "    per_epoch_trainloss = []\n",
        "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        network.zero_grad()\n",
        "        output = network(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_acc.append(loss.item())\n",
        "        current_step += 1\n",
        "        train_steps.append(current_step)\n",
        "        per_epoch_trainloss.append(loss.item())\n",
        "\n",
        "\n",
        "    network.eval()\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        for data, target in test_dataloader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            output = network(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "\n",
        "    test_loss /= len(test_dataloader.dataset)\n",
        "    accuracy = 100. * correct / len(test_dataloader.dataset)\n",
        "\n",
        "    test_loss_acc.append(test_loss)\n",
        "    test_accuracy_acc.append(accuracy)\n",
        "    test_steps.append(current_step)\n",
        "\n",
        "    wandb.log({\n",
        "        'test_loss': test_loss,\n",
        "        'test_accuracy': accuracy,\n",
        "        'train_loss': np.mean(per_epoch_trainloss)\n",
        "    })\n",
        "\n",
        "run.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4Seezpk-nHj"
      },
      "source": [
        "Pogledajmo rezultate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WYihK8r-nHj"
      },
      "source": [
        "rolling_average_width_percentage = 0.05\n",
        "plt.figure(figsize=(16, 4))\n",
        "plt.plot(train_steps, loss_acc, label=\"Train loss\")\n",
        "plt.plot(test_steps, test_loss_acc, label=\"Test loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFFHUMdo-nHj"
      },
      "source": [
        "plt.figure(figsize=(16, 4))\n",
        "plt.plot(test_steps, test_accuracy_acc, label=\"Test accuracy\")\n",
        "plt.ylabel(\"Accuracy [%]\")\n",
        "plt.xlabel(\"Training step\")\n",
        "plt.grid()\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxvzJYRw-nHj"
      },
      "source": [
        "Jedna od glavnih snaga neuronskih mreža je iskorištavanje nelinearnosti. Provedimo naš eksperiment ponovo, ali ovaj put koristeći sigmoidu kao aktivacijsku funkciju."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JJAotJd-nHk"
      },
      "source": [
        "batch_size = 4\n",
        "\n",
        "tensor_x_train = torch.Tensor(np.dstack([data_x_train, data_y_train]).reshape(number_of_train_samples, 2).astype(np.float32))\n",
        "tensor_y_train = torch.Tensor(labels_train).to(dtype=torch.int64)\n",
        "\n",
        "train_dataset = TensorDataset(tensor_x_train, tensor_y_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "tensor_x_test = torch.Tensor(np.dstack([data_x_test, data_y_test]).reshape(number_of_test_samples, 2).astype(np.float32))\n",
        "tensor_y_test = torch.Tensor(labels_test).to(dtype=torch.int64)\n",
        "\n",
        "test_dataset = TensorDataset(tensor_x_test, tensor_y_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8tcu1ZD-nHk"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 2)\n",
        "        self.fc2 = nn.Linear(2, 5)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.sigmoid(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUqwjd-3-nHl"
      },
      "source": [
        "n_epochs = 5\n",
        "learning_rate = 0.01\n",
        "device = 'cpu'\n",
        "\n",
        "network = Net().to(device)\n",
        "optimizer = optim.SGD(network.parameters(), lr=learning_rate)\n",
        "summary(network, input_size=(2, ), device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmYEBrGT-nHl"
      },
      "source": [
        "run = wandb.init(project=\"oi_demo\", reinit=True)\n",
        "wandb.watch(network, log_freq=1)\n",
        "\n",
        "loss_acc = []\n",
        "test_loss_acc = []\n",
        "test_accuracy_acc = []\n",
        "train_steps = []\n",
        "test_steps = []\n",
        "current_step = 0\n",
        "\n",
        "for epoch_idx in range(0, n_epochs):\n",
        "    network.train()\n",
        "\n",
        "    per_epoch_trainloss = []\n",
        "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        network.zero_grad()\n",
        "        output = network(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "\n",
        "        if epoch_idx == 0:\n",
        "            if batch_idx == 10:\n",
        "                print(\"# Epoha 1, batch 10\")\n",
        "                print(\"## Sloj 1 u 1. koraku:\")\n",
        "                print(\"### Tezine\")\n",
        "                print(network.fc1.weight)\n",
        "                print(\"### Tezine - Gradient\")\n",
        "                print(network.fc1.weight.grad)\n",
        "\n",
        "                print(\"### Bias\")\n",
        "                print(network.fc1.bias)\n",
        "                print(\"### Bias - Gradient\")\n",
        "                print(network.fc1.bias.grad)\n",
        "\n",
        "            if batch_idx == 11:\n",
        "                print(\"--------------\")\n",
        "                print(\"# Epoha 1, batch 11\")\n",
        "                print(\"## Sloj 1 u 1. koraku:\")\n",
        "                print(\"### Tezine\")\n",
        "                print(network.fc1.weight)\n",
        "\n",
        "                print(\"### Bias\")\n",
        "                print(network.fc1.bias)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_acc.append(loss.item())\n",
        "        current_step += 1\n",
        "        train_steps.append(current_step)\n",
        "        per_epoch_trainloss.append(loss.item())\n",
        "\n",
        "\n",
        "    network.eval()\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        for data, target in test_dataloader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            output = network(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "\n",
        "    test_loss /= len(test_dataloader.dataset)\n",
        "    accuracy = 100. * correct / len(test_dataloader.dataset)\n",
        "\n",
        "    test_loss_acc.append(test_loss)\n",
        "    test_accuracy_acc.append(accuracy)\n",
        "    test_steps.append(current_step)\n",
        "\n",
        "    wandb.log({\n",
        "        'test_loss': test_loss,\n",
        "        'test_accuracy': accuracy,\n",
        "        'train_loss': np.mean(per_epoch_trainloss)\n",
        "    })\n",
        "\n",
        "run.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DE1Ax5bS-nHl"
      },
      "source": [
        "rolling_average_width_percentage = 0.05\n",
        "plt.figure(figsize=(16, 4))\n",
        "plt.plot(train_steps, loss_acc, label=\"Raw train loss\")\n",
        "plt.plot(test_steps, test_loss_acc, label=\"Raw test loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AmLh11D-nHo"
      },
      "source": [
        "plt.figure(figsize=(16, 4))\n",
        "plt.plot(test_steps, test_accuracy_acc, label=\"Test accuracy\")\n",
        "plt.ylabel(\"Accuracy [%]\")\n",
        "plt.xlabel(\"Training step\")\n",
        "plt.grid()\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGmAmFEo-nHo"
      },
      "source": [
        "Sigmoida ima problem - u zasićenju su gradijenti gotovo 0. To je problem kada koristimo gradijentni spust kao optimizacijsku metodu! Probajmo danas najkorišteniju aktivacijsku funkciju: ReLU! ReLU je funkcija koja je obični pravac za x > 0, a 0 za sve vrijednosti manje ili jednake nuli."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2G7ivdc-nHp"
      },
      "source": [
        "batch_size = 4\n",
        "\n",
        "tensor_x_train = torch.Tensor(np.dstack([data_x_train, data_y_train]).reshape(number_of_train_samples, 2).astype(np.float32))\n",
        "tensor_y_train = torch.Tensor(labels_train).to(dtype=torch.int64)\n",
        "\n",
        "train_dataset = TensorDataset(tensor_x_train, tensor_y_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "tensor_x_test = torch.Tensor(np.dstack([data_x_test, data_y_test]).reshape(number_of_test_samples, 2).astype(np.float32))\n",
        "tensor_y_test = torch.Tensor(labels_test).to(dtype=torch.int64)\n",
        "\n",
        "test_dataset = TensorDataset(tensor_x_test, tensor_y_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlaFd57D-nHp"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 2)\n",
        "        self.fc2 = nn.Linear(2, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkCjvOwP-nHp"
      },
      "source": [
        "n_epochs = 5\n",
        "learning_rate = 0.01\n",
        "device = 'cpu'\n",
        "\n",
        "network = Net().to(device)\n",
        "optimizer = optim.SGD(network.parameters(), lr=learning_rate)\n",
        "summary(network, input_size=(2, ), device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kWRegWq-nHq"
      },
      "source": [
        "run = wandb.init(project=\"oi_demo\", reinit=True)\n",
        "wandb.watch(network, log_freq=1)\n",
        "\n",
        "loss_acc = []\n",
        "test_loss_acc = []\n",
        "test_accuracy_acc = []\n",
        "train_steps = []\n",
        "test_steps = []\n",
        "current_step = 0\n",
        "\n",
        "for epoch_idx in range(0, n_epochs):\n",
        "    network.train()\n",
        "\n",
        "    per_epoch_trainloss = []\n",
        "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        network.zero_grad()\n",
        "        output = network(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_acc.append(loss.item())\n",
        "        current_step += 1\n",
        "        train_steps.append(current_step)\n",
        "        per_epoch_trainloss.append(loss.item())\n",
        "\n",
        "\n",
        "    network.eval()\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        for data, target in test_dataloader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            output = network(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "\n",
        "    test_loss /= len(test_dataloader.dataset)\n",
        "    accuracy = 100. * correct / len(test_dataloader.dataset)\n",
        "\n",
        "    test_loss_acc.append(test_loss)\n",
        "    test_accuracy_acc.append(accuracy)\n",
        "    test_steps.append(current_step)\n",
        "\n",
        "    wandb.log({\n",
        "        'test_loss': test_loss,\n",
        "        'test_accuracy': accuracy,\n",
        "        'train_loss': np.mean(per_epoch_trainloss)\n",
        "    })\n",
        "\n",
        "run.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3a3yxL7-nHq"
      },
      "source": [
        "Vizualizirajmo rezultate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQXzUnz4-nHr"
      },
      "source": [
        "rolling_average_width_percentage = 0.05\n",
        "plt.figure(figsize=(16, 4))\n",
        "plt.plot(train_steps, loss_acc, label=\"Train loss\")\n",
        "plt.plot(test_steps, test_loss_acc, label=\"Test loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPAIObw1-nHr"
      },
      "source": [
        "plt.figure(figsize=(16, 4))\n",
        "plt.plot(test_steps, test_accuracy_acc, label=\"Test accuracy\")\n",
        "plt.ylabel(\"Accuracy [%]\")\n",
        "plt.xlabel(\"Training step\")\n",
        "plt.grid()\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UslgLIKD-nHr"
      },
      "source": [
        "Provjerimo što se dešava ako modelu sa sigmoid aktivacijom damo više vremena."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1quJPrz2-nHr"
      },
      "source": [
        "Napomena: ovaj model povremeno divergira, zbog problema kojeg smo gore naveli. Unutar 2-3 runa obično bude barem jedan dobri."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjXiEXf1-nHr"
      },
      "source": [
        "batch_size = 4\n",
        "\n",
        "tensor_x_train = torch.Tensor(np.dstack([data_x_train, data_y_train]).reshape(number_of_train_samples, 2).astype(np.float32))\n",
        "tensor_y_train = torch.Tensor(labels_train).to(dtype=torch.int64)\n",
        "\n",
        "train_dataset = TensorDataset(tensor_x_train, tensor_y_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "tensor_x_test = torch.Tensor(np.dstack([data_x_test, data_y_test]).reshape(number_of_test_samples, 2).astype(np.float32))\n",
        "tensor_y_test = torch.Tensor(labels_test).to(dtype=torch.int64)\n",
        "\n",
        "test_dataset = TensorDataset(tensor_x_test, tensor_y_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gN-iSA7L-nHr"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 8)\n",
        "        self.fc2 = nn.Linear(8, 5)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.sigmoid(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiuC6XjO-nHs"
      },
      "source": [
        "n_epochs = 10\n",
        "learning_rate = 0.01\n",
        "device = 'cpu'\n",
        "\n",
        "network = Net().to(device)\n",
        "optimizer = optim.SGD(network.parameters(), lr=learning_rate)\n",
        "summary(network, input_size=(2, ), device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC3vavYB-nHs"
      },
      "source": [
        "run = wandb.init(project=\"oi_demo\", reinit=True)\n",
        "wandb.watch(network, log_freq=1)\n",
        "\n",
        "loss_acc = []\n",
        "test_loss_acc = []\n",
        "test_accuracy_acc = []\n",
        "train_steps = []\n",
        "test_steps = []\n",
        "current_step = 0\n",
        "\n",
        "for epoch_idx in range(0, n_epochs):\n",
        "    network.train()\n",
        "\n",
        "    per_epoch_trainloss = []\n",
        "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        network.zero_grad()\n",
        "        output = network(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_acc.append(loss.item())\n",
        "        current_step += 1\n",
        "        train_steps.append(current_step)\n",
        "        per_epoch_trainloss.append(loss.item())\n",
        "\n",
        "\n",
        "    network.eval()\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        for data, target in test_dataloader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            output = network(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "\n",
        "    test_loss /= len(test_dataloader.dataset)\n",
        "    accuracy = 100. * correct / len(test_dataloader.dataset)\n",
        "\n",
        "    test_loss_acc.append(test_loss)\n",
        "    test_accuracy_acc.append(accuracy)\n",
        "    test_steps.append(current_step)\n",
        "\n",
        "    wandb.log({\n",
        "        'test_loss': test_loss,\n",
        "        'test_accuracy': accuracy,\n",
        "        'train_loss': np.mean(per_epoch_trainloss)\n",
        "    })\n",
        "\n",
        "run.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNhBJ8Hs-nHs"
      },
      "source": [
        "rolling_average_width_percentage = 0.05\n",
        "plt.figure(figsize=(16, 4))\n",
        "plt.plot(train_steps, loss_acc, label=\"Raw train loss\")\n",
        "# plt.plot(train_steps[int(len(loss_acc) - len(loss_acc) * (1-rolling_average_width_percentage)) - 1:],\n",
        "#          moving_average(loss_acc, int(len(loss_acc) * rolling_average_width_percentage)),\n",
        "#          label=f\"Smoothed ({rolling_average_width_percentage * 100:.2f}%) train loss\")\n",
        "\n",
        "plt.plot(test_steps, test_loss_acc, label=\"Raw test loss\")\n",
        "# plt.plot(test_steps[int(len(test_loss_acc) - len(test_loss_acc) * (1-rolling_average_width_percentage)) - 1:],\n",
        "#          moving_average(test_loss_acc, int(len(test_loss_acc) * rolling_average_width_percentage)),\n",
        "#          label=f\"Smoothed ({rolling_average_width_percentage * 100:.2f}%) train loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7j2ogvc-nHs"
      },
      "source": [
        "plt.figure(figsize=(16, 4))\n",
        "plt.plot(test_steps, test_accuracy_acc, label=\"Test accuracy\")\n",
        "plt.ylabel(\"Accuracy [%]\")\n",
        "plt.xlabel(\"Training step\")\n",
        "plt.grid()\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvbtH2d1-nHt"
      },
      "source": [
        "Napredak! Provjerimo sad što se dešava ako treniramo duže."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdKp_BZU-nHu"
      },
      "source": [
        "batch_size = 4\n",
        "\n",
        "tensor_x_train = torch.Tensor(np.dstack([data_x_train, data_y_train]).reshape(number_of_train_samples, 2).astype(np.float32))\n",
        "tensor_y_train = torch.Tensor(labels_train).to(dtype=torch.int64)\n",
        "\n",
        "train_dataset = TensorDataset(tensor_x_train, tensor_y_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "tensor_x_test = torch.Tensor(np.dstack([data_x_test, data_y_test]).reshape(number_of_test_samples, 2).astype(np.float32))\n",
        "tensor_y_test = torch.Tensor(labels_test).to(dtype=torch.int64)\n",
        "\n",
        "test_dataset = TensorDataset(tensor_x_test, tensor_y_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGXkXc1h-nHu"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 8)\n",
        "        self.fc2 = nn.Linear(8, 5)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.sigmoid(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7bCIt7f-nHu"
      },
      "source": [
        "n_epochs = 50\n",
        "learning_rate = 0.01\n",
        "device = 'cpu'\n",
        "\n",
        "network = Net().to(device)\n",
        "optimizer = optim.SGD(network.parameters(), lr=learning_rate)\n",
        "summary(network, input_size=(2, ), device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyuPF8TM-nHu"
      },
      "source": [
        "run = wandb.init(project=\"oi_demo\", reinit=True)\n",
        "wandb.watch(network, log_freq=1)\n",
        "\n",
        "loss_acc = []\n",
        "test_loss_acc = []\n",
        "test_accuracy_acc = []\n",
        "train_steps = []\n",
        "test_steps = []\n",
        "current_step = 0\n",
        "\n",
        "for epoch_idx in range(0, n_epochs):\n",
        "    network.train()\n",
        "\n",
        "    per_epoch_trainloss = []\n",
        "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        network.zero_grad()\n",
        "        output = network(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_acc.append(loss.item())\n",
        "        current_step += 1\n",
        "        train_steps.append(current_step)\n",
        "        per_epoch_trainloss.append(loss.item())\n",
        "\n",
        "\n",
        "    network.eval()\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        for data, target in test_dataloader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            output = network(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "\n",
        "    test_loss /= len(test_dataloader.dataset)\n",
        "    accuracy = 100. * correct / len(test_dataloader.dataset)\n",
        "\n",
        "    test_loss_acc.append(test_loss)\n",
        "    test_accuracy_acc.append(accuracy)\n",
        "    test_steps.append(current_step)\n",
        "\n",
        "    wandb.log({\n",
        "        'test_loss': test_loss,\n",
        "        'test_accuracy': accuracy,\n",
        "        'train_loss': np.mean(per_epoch_trainloss)\n",
        "    })\n",
        "\n",
        "run.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCRD-Wgt-nHu"
      },
      "source": [
        "rolling_average_width_percentage = 0.05\n",
        "plt.figure(figsize=(16, 4))\n",
        "plt.plot(train_steps, loss_acc, label=\"Raw train loss\")\n",
        "plt.plot(test_steps, test_loss_acc, label=\"Raw test loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJYU8Pm9-nHv"
      },
      "source": [
        "plt.figure(figsize=(16, 4))\n",
        "plt.plot(test_steps, test_accuracy_acc, label=\"Test accuracy\")\n",
        "plt.ylabel(\"Accuracy [%]\")\n",
        "plt.xlabel(\"Training step\")\n",
        "plt.grid()\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CH5wlsL-nHw"
      },
      "source": [
        "I konačno, isprobajmo što se dogodi ako dodamo još dva sloja na naš najuspješniji model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGe9sg80-nHw"
      },
      "source": [
        "batch_size = 4\n",
        "\n",
        "tensor_x_train = torch.Tensor(np.dstack([data_x_train, data_y_train]).reshape(number_of_train_samples, 2).astype(np.float32))\n",
        "tensor_y_train = torch.Tensor(labels_train).to(dtype=torch.int64)\n",
        "\n",
        "train_dataset = TensorDataset(tensor_x_train, tensor_y_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "tensor_x_test = torch.Tensor(np.dstack([data_x_test, data_y_test]).reshape(number_of_test_samples, 2).astype(np.float32))\n",
        "tensor_y_test = torch.Tensor(labels_test).to(dtype=torch.int64)\n",
        "\n",
        "test_dataset = TensorDataset(tensor_x_test, tensor_y_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5w_x3v_I-nHw"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 16)\n",
        "        self.fc2 = nn.Linear(16, 8)\n",
        "        self.fc3 = nn.Linear(8, 8)\n",
        "        self.fc4 = nn.Linear(8, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "\n",
        "        x = self.fc4(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvLFI6XF-nHw"
      },
      "source": [
        "n_epochs = 50\n",
        "learning_rate = 0.01\n",
        "device = 'cpu'\n",
        "\n",
        "network = Net().to(device)\n",
        "optimizer = optim.SGD(network.parameters(), lr=learning_rate)\n",
        "summary(network, input_size=(2, ), device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZL98nwG-nHw"
      },
      "source": [
        "run = wandb.init(project=\"oi_demo\", reinit=True)\n",
        "wandb.watch(network, log_freq=1)\n",
        "\n",
        "loss_acc = []\n",
        "test_loss_acc = []\n",
        "test_accuracy_acc = []\n",
        "train_steps = []\n",
        "test_steps = []\n",
        "current_step = 0\n",
        "\n",
        "for epoch_idx in range(0, n_epochs):\n",
        "    network.train()\n",
        "\n",
        "    per_epoch_trainloss = []\n",
        "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        network.zero_grad()\n",
        "        output = network(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_acc.append(loss.item())\n",
        "        current_step += 1\n",
        "        train_steps.append(current_step)\n",
        "        per_epoch_trainloss.append(loss.item())\n",
        "\n",
        "\n",
        "    network.eval()\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        for data, target in test_dataloader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            output = network(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "\n",
        "    test_loss /= len(test_dataloader.dataset)\n",
        "    accuracy = 100. * correct / len(test_dataloader.dataset)\n",
        "\n",
        "    test_loss_acc.append(test_loss)\n",
        "    test_accuracy_acc.append(accuracy)\n",
        "    test_steps.append(current_step)\n",
        "\n",
        "    wandb.log({\n",
        "        'test_loss': test_loss,\n",
        "        'test_accuracy': accuracy,\n",
        "        'train_loss': np.mean(per_epoch_trainloss)\n",
        "    })\n",
        "\n",
        "run.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbecnsfQ-nHw"
      },
      "source": [
        "rolling_average_width_percentage = 0.05\n",
        "plt.figure(figsize=(16, 4))\n",
        "plt.plot(train_steps, loss_acc, label=\"Raw train loss\")\n",
        "plt.plot(test_steps, test_loss_acc, label=\"Raw test loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9Oyr9Me-nHw"
      },
      "source": [
        "plt.figure(figsize=(16, 4))\n",
        "plt.plot(test_steps, test_accuracy_acc, label=\"Test accuracy\")\n",
        "plt.ylabel(\"Accuracy [%]\")\n",
        "plt.xlabel(\"Training step\")\n",
        "plt.grid()\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G5N6VeF-nHx"
      },
      "source": [
        "Ovaj pristup se čini dobrim i donekle stabilnim. Jedino što nam još preostaje je povećati veličinu mini-skupa. Ponovimo prošli eksperiment sa mini-skupom veličine 20."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6LL7Huz-nHx"
      },
      "source": [
        "batch_size = 20\n",
        "\n",
        "tensor_x_train = torch.Tensor(np.dstack([data_x_train, data_y_train]).reshape(number_of_train_samples, 2).astype(np.float32))\n",
        "tensor_y_train = torch.Tensor(labels_train).to(dtype=torch.int64)\n",
        "\n",
        "train_dataset = TensorDataset(tensor_x_train, tensor_y_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "tensor_x_test = torch.Tensor(np.dstack([data_x_test, data_y_test]).reshape(number_of_test_samples, 2).astype(np.float32))\n",
        "tensor_y_test = torch.Tensor(labels_test).to(dtype=torch.int64)\n",
        "\n",
        "test_dataset = TensorDataset(tensor_x_test, tensor_y_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3H98xV_-nHx"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 16)\n",
        "        self.fc2 = nn.Linear(16, 8)\n",
        "        self.fc3 = nn.Linear(8, 8)\n",
        "        self.fc4 = nn.Linear(8, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = F.relu(x, inplace=True)\n",
        "\n",
        "        x = self.fc4(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6jjXpge-nHy"
      },
      "source": [
        "n_epochs = 50\n",
        "learning_rate = 0.01\n",
        "device = 'cpu'\n",
        "\n",
        "network = Net().to(device)\n",
        "optimizer = optim.SGD(network.parameters(), lr=learning_rate)\n",
        "summary(network, input_size=(2, ), device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6HGdRxw-nHy"
      },
      "source": [
        "run = wandb.init(project=\"oi_demo\", reinit=True)\n",
        "wandb.watch(network, log_freq=1)\n",
        "\n",
        "loss_acc = []\n",
        "test_loss_acc = []\n",
        "test_accuracy_acc = []\n",
        "train_steps = []\n",
        "test_steps = []\n",
        "current_step = 0\n",
        "\n",
        "for epoch_idx in range(0, n_epochs):\n",
        "    network.train()\n",
        "\n",
        "    per_epoch_trainloss = []\n",
        "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        network.zero_grad()\n",
        "        output = network(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_acc.append(loss.item())\n",
        "        current_step += 1\n",
        "        train_steps.append(current_step)\n",
        "        per_epoch_trainloss.append(loss.item())\n",
        "\n",
        "\n",
        "    network.eval()\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        for data, target in test_dataloader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            output = network(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "\n",
        "    test_loss /= len(test_dataloader.dataset)\n",
        "    accuracy = 100. * correct / len(test_dataloader.dataset)\n",
        "\n",
        "    test_loss_acc.append(test_loss)\n",
        "    test_accuracy_acc.append(accuracy)\n",
        "    test_steps.append(current_step)\n",
        "\n",
        "    wandb.log({\n",
        "        'test_loss': test_loss,\n",
        "        'test_accuracy': accuracy,\n",
        "        'train_loss': np.mean(per_epoch_trainloss)\n",
        "    })\n",
        "\n",
        "run.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTRxt_tp-nHy"
      },
      "source": [
        "rolling_average_width_percentage = 0.05\n",
        "plt.figure(figsize=(16, 4))\n",
        "plt.plot(train_steps, loss_acc, label=\"Raw train loss\")\n",
        "# plt.plot(train_steps[int(len(loss_acc) - len(loss_acc) * (1-rolling_average_width_percentage)) - 1:],\n",
        "#          moving_average(loss_acc, int(len(loss_acc) * rolling_average_width_percentage)),\n",
        "#          label=f\"Smoothed ({rolling_average_width_percentage * 100:.2f}%) train loss\")\n",
        "\n",
        "plt.plot(test_steps, test_loss_acc, label=\"Raw test loss\")\n",
        "# plt.plot(test_steps[int(len(test_loss_acc) - len(test_loss_acc) * (1-rolling_average_width_percentage)) - 1:],\n",
        "#          moving_average(test_loss_acc, int(len(test_loss_acc) * rolling_average_width_percentage)),\n",
        "#          label=f\"Smoothed ({rolling_average_width_percentage * 100:.2f}%) train loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9Cobea3-nHz"
      },
      "source": [
        "plt.figure(figsize=(16, 4))\n",
        "plt.plot(test_steps, test_accuracy_acc, label=\"Test accuracy\")\n",
        "plt.ylabel(\"Accuracy [%]\")\n",
        "plt.xlabel(\"Training step\")\n",
        "plt.grid()\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0sx0G2x-nH0"
      },
      "source": [
        "Vidimo sad da imamo stabilnije treniranje i solidne rezultate. Na istraživaću je sad naći dobar skup hiperparametara za model (arhitektura, uvjeti treniranja i slično) da dobije najbolji mogući rezultat na neviđenom skupu podataka."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5D2bn6r-nH1"
      },
      "source": [
        "## Primjer klasifikacije slike"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFdXjPvr-nH2"
      },
      "source": [
        "Za ovaj primjer ćemo imati samo dvije klase. Imamo slike trokuta i kružića, i cilj nam je napraviti neuronsku mrežu koja može razlikovati kružiće od trokuta. Radi toga pišemo funkcije `drawRandomTriangle()` i `drawRandomCircle()`, koje nam vraćaju sliku trokuta i kružnice slučajne veličine i pozicije na slici veličine 128x128px."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UiVEoFR-nH2"
      },
      "source": [
        "def drawRandomTriangle():\n",
        "    canvas = np.zeros((128, 128, 3))\n",
        "    pt1 = (np.random.uniform(0, 128, 2).astype(np.uint64))\n",
        "    pt2 = (np.random.uniform(0, 128, 2).astype(np.uint64))\n",
        "    pt3 = (np.random.uniform(0, 128, 2).astype(np.uint64))\n",
        "\n",
        "\n",
        "    triangle_cnt = np.array( [pt1, pt2, pt3] )\n",
        "    canvas = cv2.drawContours(canvas, [triangle_cnt], 0, (255,255,255), -1)\n",
        "    return canvas[..., 0].reshape(1, 128, 128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S6b-EW3-nH2"
      },
      "source": [
        "def drawRandomCircle():\n",
        "    canvas = np.zeros((128, 128, 3))\n",
        "    radius = int(np.random.uniform(10, 40))\n",
        "    pt = tuple(np.random.uniform(0+radius, 128-radius, 2).astype(np.uint64))\n",
        "\n",
        "    canvas = cv2.circle(canvas, pt, radius, (255, 255, 255), -1)\n",
        "    return canvas[..., 0].reshape(1, 128, 128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93L-sPbF-nH2"
      },
      "source": [
        "Skup podataka se generira vrlo slično kao u prošlom zadatku, s razlikom da sada koristimo funkcije za trokut i kružić, umjesto funkcije za slučajni uzorak iz normalne distribucije."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9fhtUjs-nH3"
      },
      "source": [
        "num_of_samples_per_class = 128\n",
        "train_test_ratio = 0.8\n",
        "\n",
        "number_of_train_samples = int(2 * num_of_samples_per_class * train_test_ratio)\n",
        "number_of_test_samples = 2 * num_of_samples_per_class - number_of_train_samples\n",
        "\n",
        "data_samples_train = []\n",
        "labels_train = []\n",
        "\n",
        "data_samples_test = []\n",
        "labels_test = []\n",
        "\n",
        "for idx in range(0, number_of_train_samples):\n",
        "    data_samples_train.append(drawRandomTriangle())\n",
        "    labels_train.append(0)\n",
        "\n",
        "    data_samples_train.append(drawRandomCircle())\n",
        "    labels_train.append(1)\n",
        "\n",
        "\n",
        "for idx in range(0, number_of_test_samples):\n",
        "    data_samples_test.append(drawRandomTriangle())\n",
        "    labels_test.append(0)\n",
        "\n",
        "    data_samples_test.append(drawRandomCircle())\n",
        "    labels_test.append(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktR0dZFy-nH3"
      },
      "source": [
        "Vizualizirajmo naš skup podataka."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sGu-1s3-nH3"
      },
      "source": [
        "plt.figure(figsize=(32, 32))\n",
        "for idx in range(0, 100):\n",
        "    plt.subplot(10, 10, idx + 1)\n",
        "    plt.imshow(data_samples_train[idx][0, ...])\n",
        "    plt.axis('off')\n",
        "    plt.colorbar()\n",
        "    plt.title(labels_train[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svykVeor-nH5"
      },
      "source": [
        "Izrada `DataLoader` je ista kao u prethodnim koracima. Odlučili smo se na veličinu mini-skupa od 16."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "875Q5f0Z-nH5"
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "tensor_x_train = torch.Tensor(data_samples_train)\n",
        "tensor_y_train = torch.Tensor(labels_train).to(dtype=torch.int64)\n",
        "\n",
        "train_dataset = TensorDataset(tensor_x_train, tensor_y_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "tensor_x_test = torch.Tensor(data_samples_test)\n",
        "tensor_y_test = torch.Tensor(labels_test).to(dtype=torch.int64)\n",
        "\n",
        "test_dataset = TensorDataset(tensor_x_test, tensor_y_test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyBjTOQ_-nH6"
      },
      "source": [
        "Ovo je jedan predloženi model. Ovdje se možete poigrati s parametrima da vidite utjecaj na rezultat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmODEebz-nH6"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 4, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(4, 8, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(8, 4, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(1024, 2)\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = x.view(-1, 1024)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM878SIg-nH6"
      },
      "source": [
        "n_epochs = 10\n",
        "learning_rate = 0.01\n",
        "device = 'cpu'\n",
        "\n",
        "network = Net().to(device)\n",
        "optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
        "summary(network, input_size=(1, 128, 128), device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glzNbfsD-nH7"
      },
      "source": [
        "run = wandb.init(project=\"oi_demo\", reinit=True)\n",
        "wandb.watch(network, log_freq=1)\n",
        "\n",
        "loss_acc = []\n",
        "test_loss_acc = []\n",
        "test_accuracy_acc = []\n",
        "train_steps = []\n",
        "test_steps = []\n",
        "current_step = 0\n",
        "\n",
        "for epoch_idx in tqdm.tqdm(range(0, n_epochs)):\n",
        "    network.train()\n",
        "\n",
        "    per_epoch_trainloss = []\n",
        "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        network.zero_grad()\n",
        "        output = network(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_acc.append(loss.item())\n",
        "        current_step += 1\n",
        "        train_steps.append(current_step)\n",
        "        per_epoch_trainloss.append(loss.item())\n",
        "\n",
        "\n",
        "    network.eval()\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        for data, target in test_dataloader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            output = network(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "\n",
        "    test_loss /= len(test_dataloader.dataset)\n",
        "    accuracy = 100. * correct / len(test_dataloader.dataset)\n",
        "\n",
        "    test_loss_acc.append(test_loss)\n",
        "    test_accuracy_acc.append(accuracy)\n",
        "    test_steps.append(current_step)\n",
        "\n",
        "    wandb.log({\n",
        "        'test_loss': test_loss,\n",
        "        'test_accuracy': accuracy,\n",
        "        'train_loss': np.mean(per_epoch_trainloss)\n",
        "    })\n",
        "\n",
        "run.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bnHAozx-nH7"
      },
      "source": [
        "Vizualizirajmo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFBXJGtK-nH7"
      },
      "source": [
        "rolling_average_width_percentage = 0.05\n",
        "plt.figure(figsize=(16, 4))\n",
        "\n",
        "plt.plot(train_steps, loss_acc, label=\"Raw train loss\")\n",
        "plt.plot(test_steps, test_loss_acc, label=\"Raw test loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PODgMYi6-nH8"
      },
      "source": [
        "plt.figure(figsize=(16, 4))\n",
        "plt.plot(test_steps, test_accuracy_acc, label=\"Test accuracy\")\n",
        "plt.ylabel(\"Accuracy [%]\")\n",
        "plt.xlabel(\"Training step\")\n",
        "plt.grid()\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9lA8pxf-nH8"
      },
      "source": [
        "Ovdje možemo primjetiti jedan zanimljivi fenomen: **overfitting**. Imamo problem da iz početka train i test loss padaju, a nakon nekog vremena test loss krene rasti, dok train loss i dalje pada. To znači da naša mreža više ne uči neki generalizirani način za prepoznavanje, nego **pamti konkretne primjere**. To nije dobro, i overfitting želimo po svaku cijenu spriječiti."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xJcomCM-nH8"
      },
      "source": [
        "Da bi spriječili overfitting, možemo mrežu produbiti. Na taj način smanjujemo prostornu veličinu (visina i širina tenzora), a povećavamo dubinu (broj filtera u zadnjoj konvoluciji; \"broj značajki\"). Proširujemo model sa dvije dodatne konvolucije, i povećavamo broj filtera u svim konvolucijama."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPtz4Wcb-nH8"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(1024, 2)\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.conv5(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = x.view(-1, 1024)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6uWHCrR-nH9"
      },
      "source": [
        "n_epochs = 10\n",
        "learning_rate = 0.01\n",
        "device = 'cpu'\n",
        "\n",
        "network = Net().to(device)\n",
        "optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
        "summary(network, input_size=(1, 128, 128), device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeplk1aK-nH9"
      },
      "source": [
        "run = wandb.init(project=\"oi_demo\", reinit=True)\n",
        "wandb.watch(network, log_freq=1)\n",
        "\n",
        "loss_acc = []\n",
        "test_loss_acc = []\n",
        "test_accuracy_acc = []\n",
        "train_steps = []\n",
        "test_steps = []\n",
        "current_step = 0\n",
        "\n",
        "for epoch_idx in tqdm.tqdm(range(0, n_epochs)):\n",
        "    network.train()\n",
        "\n",
        "    per_epoch_trainloss = []\n",
        "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        network.zero_grad()\n",
        "        output = network(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_acc.append(loss.item())\n",
        "        current_step += 1\n",
        "        train_steps.append(current_step)\n",
        "        per_epoch_trainloss.append(loss.item())\n",
        "\n",
        "\n",
        "    network.eval()\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        for data, target in test_dataloader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            output = network(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "\n",
        "    test_loss /= len(test_dataloader.dataset)\n",
        "    accuracy = 100. * correct / len(test_dataloader.dataset)\n",
        "\n",
        "    test_loss_acc.append(test_loss)\n",
        "    test_accuracy_acc.append(accuracy)\n",
        "    test_steps.append(current_step)\n",
        "\n",
        "    wandb.log({\n",
        "        'test_loss': test_loss,\n",
        "        'test_accuracy': accuracy,\n",
        "        'train_loss': np.mean(per_epoch_trainloss)\n",
        "    })\n",
        "\n",
        "run.finish()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_Smri0a-nH9"
      },
      "source": [
        "Vizualizirajmo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEZfjMRy-nH9"
      },
      "source": [
        "rolling_average_width_percentage = 0.05\n",
        "plt.figure(figsize=(16, 4))\n",
        "\n",
        "plt.plot(train_steps, loss_acc, label=\"Raw train loss\")\n",
        "plt.plot(test_steps, test_loss_acc, label=\"Raw test loss\")\n",
        "\n",
        "plt.legend()\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kODGOpqw-nH-"
      },
      "source": [
        "plt.figure(figsize=(16, 4))\n",
        "plt.plot(test_steps, test_accuracy_acc, label=\"Test accuracy\")\n",
        "plt.ylabel(\"Accuracy [%]\")\n",
        "plt.xlabel(\"Training step\")\n",
        "plt.grid()\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSFcXuhJ-nH_"
      },
      "source": [
        "Vidimo da mreža relativno brzo počinje generalizirati. Nema značajne razlike u trendu između train i test lossa, a accuracy na test skupu je visok, unatoč tome što su to neviđeni podaci za model. S ovime smo napravili model koji je sposoban razlikovati slike kružića od slika trokuta."
      ]
    }
  ]
}